{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the gensim model into spacy model didn't work (couldn't convert the gensim model) \n",
    "\n",
    "# import spacy\n",
    "# # Load the spacy model that you have installed\n",
    "# nlp = spacy.load('./word2vec_model_one_gesim.model/')\n",
    "# # process a sentence using the model\n",
    "# doc = nlp(\"This is some text that I am processing with Spacy\")\n",
    "# # It's that simple - all of the vectors and words are assigned after this point\n",
    "# # Get the vector for 'text':\n",
    "# doc[3].vector\n",
    "# # Get the mean vector for the entire sentence (useful for sentence classification etc.)\n",
    "# doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name:str, path:str =\"\"):\n",
    "    return gensim.models.Word2Vec.load(path + name)\n",
    "# model = gensim.models.Word2Vec.load(\"word2vec_model_one_gesim.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(name: str, path:str=\"\"):\n",
    "    return pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_question_data(data):\n",
    "    question1_data = data[\"question1\"].values.tolist()\n",
    "    question2_data = data[\"question2\"].values.tolist()\n",
    "    labels = data[\"is_duplicate\"].values.tolist()\n",
    "\n",
    "    return [question1_data, question2_data, labels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data (tokenizing and removing punctutation)\n",
    "def gensim_preprocess(question_data):\n",
    "    return [gensim.utils.simple_preprocess(str(sentence)) for sentence in question_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_preprocess(question_data):\n",
    "    stop_words=['the', 'a', 'an', 'and', 'is', 'be', 'will']\n",
    "    new_data = []\n",
    "    \n",
    "    for question in question_data:\n",
    "        processed_question = []\n",
    "        question = str(question)\n",
    "        \n",
    "        #split the question to words (doesn't include stop words)\n",
    "        words = [word.lower() for word in question.split() if word not in stop_words]\n",
    "        for word in words:\n",
    "            # to remove punctutation from string (imported from string library)\n",
    "            word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "            word = word.replace('“', '').replace('”', '')\n",
    "\n",
    "            if len(word) > 0:\n",
    "                processed_question.append(word)\n",
    "                \n",
    "        new_data.append(processed_question)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gensim = load_model(\"word2vec_model_gensim_100.model\")\n",
    "model_manual = load_model(\"word2vec_model_manual_100.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parser_f() missing 1 required positional argument: 'filepath_or_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-83bcda74ee1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-f781685df3e9>\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: parser_f() missing 1 required positional argument: 'filepath_or_buffer'"
     ]
    }
   ],
   "source": [
    "data = read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question1_data = df[\"question1\"].values.tolist()\n",
    "# question2_data = df[\"question2\"].values.tolist()\n",
    "# label = df[\"is_duplicate\"].values.tolist()\n",
    "# print(question1_data[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_question1_data = [gensim.utils.simple_preprocess(str(sentence)) for sentence in question1_data] \n",
    "# cleaned_question2_data = [gensim.utils.simple_preprocess(str(sentence)) for sentence in question2_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india'], ['what', 'is', 'the', 'story', 'of', 'kohinoor', 'koh', 'noor', 'diamond'], ['how', 'can', 'increase', 'the', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'vpn'], ['why', 'am', 'mentally', 'very', 'lonely', 'how', 'can', 'solve', 'it']]\n",
      "[['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market'], ['what', 'would', 'happen', 'if', 'the', 'indian', 'government', 'stole', 'the', 'kohinoor', 'koh', 'noor', 'diamond', 'back'], ['how', 'can', 'internet', 'speed', 'be', 'increased', 'by', 'hacking', 'through', 'dns'], ['find', 'the', 'remainder', 'when', 'math', 'math', 'is', 'divided', 'by']]\n"
     ]
    }
   ],
   "source": [
    "# print(cleaned_question1_data[0:4])\n",
    "# print(cleaned_question2_data[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28840\n"
     ]
    }
   ],
   "source": [
    "question1_word_vectors = []\n",
    "count = 1\n",
    "\n",
    "for word_list in cleaned_question1_data:\n",
    "    temp_list = []\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            temp_list.append(model.wv[word])\n",
    "        except:\n",
    "            count += 1 \n",
    "    question1_word_vectors.append(temp_list)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23361\n"
     ]
    }
   ],
   "source": [
    "question2_word_vectors = []\n",
    "count = 1\n",
    "labels = []\n",
    "for word_list in cleaned_question2_data:\n",
    "    temp_list = []\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            temp_list.append(model.wv[word])\n",
    "        except:\n",
    "            count += 1 \n",
    "    question2_word_vectors.append(temp_list)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404290\n",
      "404290\n"
     ]
    }
   ],
   "source": [
    "print(len(question1_word_vectors))\n",
    "print(len(question2_word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_sentence_vectors = [sum(sentence) for sentence in question1_word_vectors if len(sentence)>0]\n",
    "question2_sentence_vectors = [sum(sentence) for sentence in question2_word_vectors if len(sentence)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for i in range(len(question1_sentence_vectors)):\n",
    "    training_data.append(1 - spatial.distance.cosine(question1_sentence_vectors[i], question2_sentence_vectors[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.973378598690033, 0.3867042362689972, 0.7577860355377197, 0.2219158113002777, 0.8361110687255859, 0.9349866509437561, 0.08878669887781143, 0.669806182384491, 0.9755182266235352, 0.6018723845481873, 0.37011805176734924, 0.8813337683677673, 0.8732406497001648, 0.9487687945365906, 0.9977368116378784, 0.8526891469955444, 0.8565733432769775, 0.7768007516860962, 0.8304876685142517, 0.9592839479446411]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(label[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(np.array(training_data).reshape(-1,1), np.array(label[:404260]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 1.0000001192092896),\n",
       " ('which', 0.5997904539108276),\n",
       " ('the', 0.4692210257053375),\n",
       " ('some', 0.35175949335098267),\n",
       " ('globalization', 0.3471813499927521),\n",
       " ('practices', 0.34091559052467346),\n",
       " ('crusades', 0.33653372526168823),\n",
       " ('horror', 0.33569419384002686),\n",
       " ('firing', 0.3314772844314575),\n",
       " ('psychological', 0.3312871754169464)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(model.wv[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_test_data = df_test[\"question1\"].values.tolist()\n",
    "question2_test_data = df_test[\"question2\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345796\n"
     ]
    }
   ],
   "source": [
    "print(len(question1_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_question1_test_data = [gensim.utils.simple_preprocess(str(sentence)) for sentence in question1_test_data] \n",
    "cleaned_question2_test_data = [gensim.utils.simple_preprocess(str(sentence)) for sentence in question2_test_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425992\n",
      "425992\n"
     ]
    }
   ],
   "source": [
    "question1_word_test_vectors = []\n",
    "count = 1\n",
    "\n",
    "for word_list in cleaned_question1_test_data:\n",
    "    temp_list = []\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            temp_list.append(model.wv[word])\n",
    "        except:\n",
    "            count += 1 \n",
    "    question1_word_test_vectors.append(temp_list)\n",
    "print(count)\n",
    "\n",
    "question2_word_test_vectors = []\n",
    "count = 1\n",
    "labels = []\n",
    "for word_list in cleaned_question1_test_data:\n",
    "    temp_list = []\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            temp_list.append(model.wv[word])\n",
    "        except:\n",
    "            count += 1 \n",
    "    question2_word_test_vectors.append(temp_list)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_test_sentence_vectors = [sum(sentence) for sentence in question1_word_test_vectors if len(sentence)>0]\n",
    "question2_test_sentence_vectors = [sum(sentence) for sentence in question2_word_test_vectors if len(sentence)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in range(len(question1_test_sentence_vectors)):\n",
    "    test_data.append(1 - spatial.distance.cosine(question1_test_sentence_vectors[i], question2_test_sentence_vectors[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563108\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(np.array(test_data).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
