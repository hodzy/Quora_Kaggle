{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries: gensim & pyhton-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data (path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "#     df = pd.read_csv('train.csv')\n",
    "    # print(df)\n",
    "    # print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_question_data(data):\n",
    "    data = df[\"question1\"].values.tolist()\n",
    "    data.extend(df[\"question2\"].values.tolist())\n",
    "    # print(len(data))\n",
    "    # print(data[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data (tokenizing and removing punctutation)\n",
    "def gensim_preprocess(question_data):\n",
    "    new_data = [gensim.utils.simple_preprocess(str(sentence)) for sentence in question_data] \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_preprocess(question_data):\n",
    "    stop_words=['the', 'a', 'an', 'and', 'is', 'be', 'will']\n",
    "    new_data = []\n",
    "    \n",
    "    for question in question_data:\n",
    "        processed_question = []\n",
    "        question = str(question)\n",
    "        \n",
    "        #split the question to words (doesn't include stop words)\n",
    "        words = [word.lower() for word in question.split() if word not in stop_words]\n",
    "        for word in words:\n",
    "            # to remove punctutation from string (imported from string library)\n",
    "            word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "            word = word.replace('“', '').replace('”', '')\n",
    "\n",
    "            if len(word) > 0:\n",
    "                processed_question.append(word)\n",
    "                \n",
    "        new_data.append(processed_question)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default vector_size is 100\n",
    "def gensim_word2vec(training_data, window_size, min_word_count, vector_size, workers):\n",
    "    model = gensim.models.Word2Vec(window= window_size, vector_size= vector_size, \n",
    "                                   min_count= min_word_count, workers = workers)\n",
    "#     model.build_vocab(new_data, progress_per= 1000)\n",
    "    model.build_vocab(training_data, progress_per= 1000)\n",
    "    model.train(training_data, total_examples= model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "#     for handling unknown vectors\n",
    "    model.wv[\"UNK\"] = np.random.rand(vector_size)\n",
    "#     print(model.epochs)\n",
    "#     print(model.corpus_count)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"word2vec_model_one_gesim.model\")\n",
    "def save_gensim_model(model, name):\n",
    "#     model.save(\"word2vec_model_one_gensim_vector_300.model\")\n",
    "    model.save(str(name) + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(model, word:str):\n",
    "    try: \n",
    "        return model.wv.most_similar(word)\n",
    "    except:\n",
    "        return model.wv.most_similar(\"UNK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings (model ,word:str):\n",
    "#     model.wv.get_vector[word]\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        return model.wv[\"UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data('train.csv')\n",
    "data = join_question_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_manual = string_preprocess(data)\n",
    "processed_data_gensim = gensim_preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = gensim_word2vec(processed_data_manual, 5, 2, 100, 12)\n",
    "model_2 = gensim_word2vec(processed_data_gensim, 5, 2, 100, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_gensim_model(model_1, \"word2vec_model_manual_100\")\n",
    "save_gensim_model(model_2, \"word2vec_model_gensim_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('which', 0.5807263851165771), ('whats', 0.5686727166175842), ('youin', 0.45047006011009216), ('groundattack', 0.4348257780075073), ('some', 0.4311452805995941), ('what’s', 0.4174785315990448), ('materialsvideosresources', 0.41740575432777405), ('dedications', 0.39461904764175415), ('weavers', 0.3931557536125183), ('masturbationporn', 0.3927534222602844)]\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar(model_1, \"what\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('which', 0.6183813810348511), ('the', 0.4920017421245575), ('caesium', 0.3833654224872589), ('brigg', 0.37125155329704285), ('some', 0.37123170495033264), ('walkway', 0.36221301555633545), ('horror', 0.3496111333370209), ('halloween', 0.34662020206451416), ('stoping', 0.3436439335346222), ('podcasts', 0.3425482213497162)]\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar(model_2, \"what\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('schumers', 0.3628362715244293), ('hammerbacher', 0.3415323495864868), ('needs', 0.3413182199001312), ('obvious', 0.3345271050930023), ('errands', 0.3241003751754761), ('understands', 0.316386342048645), ('orchestrated', 0.3087090849876404), ('realization', 0.30723509192466736), ('adjuster', 0.30591443181037903), ('musks', 0.3045320212841034)]\n",
      "[('schumers', 0.3628362715244293), ('hammerbacher', 0.3415323495864868), ('needs', 0.3413182199001312), ('obvious', 0.3345271050930023), ('errands', 0.3241003751754761), ('understands', 0.316386342048645), ('orchestrated', 0.3087090849876404), ('realization', 0.30723509192466736), ('adjuster', 0.30591443181037903), ('musks', 0.3045320212841034)]\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar(model_1, \"vnfjnvd\"))\n",
    "print(find_most_similar(model_1, \"hsudhf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india'], ['what', 'story', 'of', 'kohinoor', 'kohinoor', 'diamond'], ['how', 'can', 'i', 'increase', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'vpn'], ['why', 'am', 'i', 'mentally', 'very', 'lonely', 'how', 'can', 'i', 'solve', 'it'], ['which', 'one', 'dissolve', 'in', 'water', 'quikly', 'sugar', 'salt', 'methane', 'carbon', 'di', 'oxide']]\n"
     ]
    }
   ],
   "source": [
    "print(processed_data_manual[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india'], ['what', 'is', 'the', 'story', 'of', 'kohinoor', 'koh', 'noor', 'diamond'], ['how', 'can', 'increase', 'the', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'vpn'], ['why', 'am', 'mentally', 'very', 'lonely', 'how', 'can', 'solve', 'it'], ['which', 'one', 'dissolve', 'in', 'water', 'quikly', 'sugar', 'salt', 'methane', 'and', 'carbon', 'di', 'oxide']]\n"
     ]
    }
   ],
   "source": [
    "print(processed_data_gensim[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
