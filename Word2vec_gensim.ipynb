{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries: gensim & pyhton-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data (path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "#     df = pd.read_csv('train.csv')\n",
    "    # print(df)\n",
    "    # print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_question_data(data):\n",
    "    data = df[\"question1\"].values.tolist()\n",
    "    data.extend(df[\"question2\"].values.tolist())\n",
    "    # print(len(data))\n",
    "    # print(data[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data (tokenizing and removing punctutation)\n",
    "def gensim_preprocess(question_data):\n",
    "    new_data = [gensim.utils.simple_preprocess(str(sentence)) for sentence in question_data] \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_preprocess(question_data):\n",
    "    stop_words=['the', 'a', 'an', 'and', 'is', 'be', 'will']\n",
    "    new_data = []\n",
    "    \n",
    "    for question in question_data:\n",
    "        processed_question = []\n",
    "        question = str(question)\n",
    "        \n",
    "        #split the question to words (doesn't include stop words)\n",
    "        words = [word.lower() for word in question.split() if word not in stop_words]\n",
    "        for word in words:\n",
    "            # to remove punctutation from string (imported from string library)\n",
    "            word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "            word = word.replace('“', '').replace('”', '')\n",
    "\n",
    "            if len(word) > 0:\n",
    "                processed_question.append(word)\n",
    "                \n",
    "        new_data.append(processed_question)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default vector_size is 100\n",
    "def gensim_word2vec(training_data, window_size, min_word_count, vector_size, workers):\n",
    "    model = gensim.models.Word2Vec(window=5, vector_size=300, min_count=3, workers = 12)\n",
    "#     model.build_vocab(new_data, progress_per= 1000)\n",
    "    model.build_vocab(training_data, progress_per= 1000)\n",
    "    model.train(training_data, total_examples= model.corpus_count, epochs=model.epochs)\n",
    "#     print(model.epochs)\n",
    "#     print(model.corpus_count)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"word2vec_model_one_gesim.model\")\n",
    "def save_gensim_model(model, name):\n",
    "#     model.save(\"word2vec_model_one_gensim_vector_300.model\")\n",
    "    model.save(str(name) + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(model, word:str):\n",
    "    return model.wv.most_similar(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings (model ,word:str):\n",
    "#     model.wv.get_vector[word]\n",
    "    return model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data('train.csv')\n",
    "data = join_question_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_manual = string_preprocess(data)\n",
    "processed_data_gensim = gensim_preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = gensim_word2vec(processed_data_manual, 5, 2, 100, 12)\n",
    "model_2 = gensim_word2vec(processed_data_gensim, 5, 2, 100, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_gensim_model(model_1, \"word2vec_model_manual_100\")\n",
    "save_gensim_model(model_2, \"word2vec_model_gensim_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('which', 0.4715254306793213), ('whats', 0.443397581577301), ('horticulture', 0.36997199058532715), ('what’s', 0.36587104201316833), ('proverbs', 0.36556464433670044), ('whatre', 0.35183969140052795), ('some', 0.35031643509864807), ('insights', 0.3402627408504486), ('practices', 0.34008562564849854), ('superstitions', 0.33570602536201477)]\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar(model_1, \"what\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('which', 0.521908700466156), ('the', 0.47264647483825684), ('exersice', 0.3212506175041199), ('some', 0.3124181628227234), ('productivity', 0.3114285469055176), ('seo', 0.30419066548347473), ('grails', 0.30178216099739075), ('horror', 0.3007189631462097), ('whta', 0.2945222854614258), ('shayari', 0.2923395037651062)]\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar(model_2, \"what\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india'], ['what', 'story', 'of', 'kohinoor', 'kohinoor', 'diamond'], ['how', 'can', 'i', 'increase', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'vpn'], ['why', 'am', 'i', 'mentally', 'very', 'lonely', 'how', 'can', 'i', 'solve', 'it'], ['which', 'one', 'dissolve', 'in', 'water', 'quikly', 'sugar', 'salt', 'methane', 'carbon', 'di', 'oxide']]\n"
     ]
    }
   ],
   "source": [
    "print(processed_data_manual[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india'], ['what', 'is', 'the', 'story', 'of', 'kohinoor', 'koh', 'noor', 'diamond'], ['how', 'can', 'increase', 'the', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'vpn'], ['why', 'am', 'mentally', 'very', 'lonely', 'how', 'can', 'solve', 'it'], ['which', 'one', 'dissolve', 'in', 'water', 'quikly', 'sugar', 'salt', 'methane', 'and', 'carbon', 'di', 'oxide']]\n"
     ]
    }
   ],
   "source": [
    "print(processed_data_gensim[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
