{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries: gensim & pyhton-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import keyedvectors\n",
    "import gensim\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data (path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "#     df = pd.read_csv('train.csv')\n",
    "    # print(df)\n",
    "    # print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_question_data(data):\n",
    "    data = df[\"question1\"].values.tolist()\n",
    "    data.extend(df[\"question2\"].values.tolist())\n",
    "    # print(len(data))\n",
    "    # print(data[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data (tokenizing and removing punctutation)\n",
    "def gensim_preprocess(question_data):\n",
    "    new_data = [gensim.utils.simple_preprocess(str(sentence)) for sentence in question_data]\n",
    "\n",
    "#     for handling unknown vectors (couldn't add after training the model)\n",
    "    new_data.append([\"UNK\",\"UNK\"])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_preprocess(question_data):\n",
    "    stop_words=['the', 'a', 'an', 'and', 'is', 'be', 'will']\n",
    "    new_data = []\n",
    "    \n",
    "    for question in question_data:\n",
    "        processed_question = []\n",
    "        question = str(question)\n",
    "        \n",
    "        #split the question to words (doesn't include stop words)\n",
    "        words = [word.lower() for word in question.split() if word not in stop_words]\n",
    "        for word in words:\n",
    "            # to remove punctutation from string (imported from string library)\n",
    "            word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "            word = word.replace('“', '').replace('”', '')\n",
    "\n",
    "            if len(word) > 0:\n",
    "                processed_question.append(word)\n",
    "                \n",
    "        new_data.append(processed_question)\n",
    "        \n",
    "#     for handling unknown vectors (couldn't add after training the model)\n",
    "    new_data.append([\"UNK\",\"UNK\"])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default vector_size is 100\n",
    "def gensim_word2vec(training_data, window_size, min_word_count, vector_size, workers):\n",
    "    model = gensim.models.Word2Vec(window= window_size, vector_size= vector_size, \n",
    "                                   min_count= min_word_count, workers = workers)\n",
    "\n",
    "#     model.build_vocab(new_data, progress_per= 1000)\n",
    "    model.build_vocab(training_data, progress_per= 1000)\n",
    "    model.train(training_data, total_examples= model.corpus_count, epochs=model.epochs)\n",
    "#     print(model.epochs)\n",
    "#     print(model.corpus_count)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"word2vec_model_one_gesim.model\")\n",
    "def save_gensim_model(model, name):\n",
    "#     model.save(\"word2vec_model_one_gensim_vector_300.model\")\n",
    "    model.save(str(name) + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(model, word:str):\n",
    "    try: \n",
    "        return model.wv.most_similar(word)\n",
    "    except:\n",
    "        return model.wv.most_similar(\"UNK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings (model ,word:str):\n",
    "#     model.wv.get_vector[word]\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        return model.wv[\"UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data('train.csv')\n",
    "data = join_question_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_manual = string_preprocess(data)\n",
    "processed_data_gensim = gensim_preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = gensim_word2vec(processed_data_manual, 5, 2, 100, 12)\n",
    "model_2 = gensim_word2vec(processed_data_gensim, 5, 2, 100, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_gensim_model(model_1, \"word2vec_model_manual_100\")\n",
    "save_gensim_model(model_2, \"word2vec_model_gensim_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('which', 0.5725353956222534), ('whats', 0.5608700513839722), ('what’s', 0.43915098905563354), ('efforttoresult', 0.42689210176467896), ('some', 0.4265798032283783), ('materialsvideosresources', 0.41167664527893066), ('deliverycouriers', 0.4001527726650238), ('psychological', 0.39648371934890747), ('stoping', 0.3740164339542389), ('presentations', 0.3732001781463623)]\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar(model_1, \"what\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('which', 0.6236398816108704), ('the', 0.46475470066070557), ('some', 0.37875428795814514), ('proverbs', 0.36545121669769287), ('graduations', 0.3605636656284332), ('basic', 0.3572506606578827), ('angkor', 0.3516005277633667), ('outlandish', 0.3498055040836334), ('crusades', 0.3423321843147278), ('horror', 0.3412609398365021)]\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar(model_2, \"what\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('é', 0.9334388375282288), ('è', 0.9314623475074768), ('reverent', 0.9307461977005005), ('ê', 0.9292716979980469), ('1114', 0.9288990497589111), ('parleg', 0.9283602237701416), ('‘life', 0.9269662499427795), ('leveltriggered', 0.9268814921379089), ('advice’', 0.9262845516204834), ('athiests', 0.9257882833480835)]\n",
      "[('é', 0.9334388375282288), ('è', 0.9314623475074768), ('reverent', 0.9307461977005005), ('ê', 0.9292716979980469), ('1114', 0.9288990497589111), ('parleg', 0.9283602237701416), ('‘life', 0.9269662499427795), ('leveltriggered', 0.9268814921379089), ('advice’', 0.9262845516204834), ('athiests', 0.9257882833480835)]\n"
     ]
    }
   ],
   "source": [
    "print(find_most_similar(model_1, \"UNK\"))\n",
    "print(find_most_similar(model_1, \"hsudhf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india'], ['what', 'story', 'of', 'kohinoor', 'kohinoor', 'diamond'], ['how', 'can', 'i', 'increase', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'vpn'], ['why', 'am', 'i', 'mentally', 'very', 'lonely', 'how', 'can', 'i', 'solve', 'it'], ['which', 'one', 'dissolve', 'in', 'water', 'quikly', 'sugar', 'salt', 'methane', 'carbon', 'di', 'oxide']]\n",
      "['UNK', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "print(processed_data_manual[0:5])\n",
    "print(processed_data_manual[len(processed_data_manual)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india'], ['what', 'is', 'the', 'story', 'of', 'kohinoor', 'koh', 'noor', 'diamond'], ['how', 'can', 'increase', 'the', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'vpn'], ['why', 'am', 'mentally', 'very', 'lonely', 'how', 'can', 'solve', 'it'], ['which', 'one', 'dissolve', 'in', 'water', 'quikly', 'sugar', 'salt', 'methane', 'and', 'carbon', 'di', 'oxide']]\n"
     ]
    }
   ],
   "source": [
    "print(processed_data_gensim[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
